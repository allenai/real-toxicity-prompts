{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from simpletransformers.classification import MultiLabelClassificationModel\n",
    "\n",
    "from constants import TOXIC_COMMENTS_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train = pd.read_csv(TOXIC_COMMENTS_DIR / 'train.csv')\n",
    "labels = list(raw_train.columns[2:])\n",
    "\n",
    "train_df = pd.DataFrame()\n",
    "train_df['text'] = raw_train['comment_text']\n",
    "train_df['labels'] = raw_train[labels].values.tolist()\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_test_comments = pd.read_csv(TOXIC_COMMENTS_DIR / 'test.csv')\n",
    "raw_test_labels = pd.read_csv(TOXIC_COMMENTS_DIR / 'test_labels.csv')\n",
    "raw_test_labels = raw_test_labels[raw_test_labels['toxic'] != -1]\n",
    "raw_test = raw_test_comments.merge(raw_test_labels)\n",
    "\n",
    "test_df = pd.DataFrame()\n",
    "test_df['text'] = raw_test['comment_text']\n",
    "test_df['labels'] = raw_test_labels[labels].values.tolist()\n",
    "\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a MultiLabelClassificationModel\n",
    "model = MultiLabelClassificationModel(\n",
    "    'distilbert',\n",
    "    'distilbert-base-uncased',\n",
    "    num_labels=len(labels),\n",
    "    args={'reprocess_input_data': True, 'overwrite_output_dir': True, 'num_train_epochs': 3}\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "model.train_model(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "result, model_outputs, wrong_predictions = model.eval_model(test_df)\n",
    "print(result)\n",
    "print(model_outputs)\n",
    "\n",
    "predictions, raw_outputs = model.predict(['This thing is entirely different from the other thing. '])\n",
    "print(predictions)\n",
    "print(raw_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, test_outputs = model.predict(raw_test_comments['comment_text'].tolist())\n",
    "\n",
    "sub_df = pd.DataFrame(test_outputs, columns=labels)\n",
    "sub_df['id'] = raw_test_comments['id']\n",
    "sub_df = sub_df[['id', 'toxic','severe_toxic','obscene','threat','insult','identity_hate']]\n",
    "sub_df.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def grouper_it(n, it):\n",
    "    while True:\n",
    "        chunk_it = itertools.islice(it, n)\n",
    "        try:\n",
    "            first_el = next(chunk_it)\n",
    "        except StopIteration:\n",
    "            return\n",
    "        yield itertools.chain((first_el,), chunk_it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNK_SIZE = 100000\n",
    "TOTAL_ROWS = 8003023\n",
    "LABELS = (TOXIC_COMMENTS_DIR / 'classes.txt').read_text().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "model = MultiLabelClassificationModel(\n",
    "    'distilbert',\n",
    "    'outputs/',\n",
    "    num_labels=len(LABELS)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "\n",
    "STORE = pd.HDFStore(DATA_DIR / 'toxicity_classifications.h5', mode='w')\n",
    "\n",
    "with tqdm_notebook(total=TOTAL_ROWS) as pbar:\n",
    "    for chunk in grouper_it(CHUNK_SIZE, TEXTS_DIR.iterdir()):\n",
    "        pbar.set_description('Loading texts')\n",
    "        paths = list(chunk)\n",
    "        texts = [file.read_text() for file in paths]\n",
    "\n",
    "        pbar.set_description('Making predictions')\n",
    "        predictions, outputs = model.predict(texts)\n",
    "        \n",
    "        pbar.set_description('Concatenating predictions to dataframe')\n",
    "        filenames = [file.name for file in paths]\n",
    "        df = pd.DataFrame(filenames, columns=['filename'])\n",
    "        df[LABELS] = pd.DataFrame(outputs)\n",
    "        \n",
    "        pbar.set_description('Appending to HDF file')\n",
    "        STORE.append('df', df, expectedrows=TOTAL_ROWS, index=False)\n",
    "            \n",
    "        pbar.update(CHUNK_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "from constants import TOXICITY_CLASSIFICATIONS_H5, TOXICITY_SCORES_PICKLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "store = pd.HDFStore(TOXICITY_CLASSIFICATIONS_H5)\n",
    "pred_df = store['df']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>text</th>\n",
       "      <th>chunk_num</th>\n",
       "      <th>toxicity</th>\n",
       "      <th>severe_toxicity</th>\n",
       "      <th>identity_attack</th>\n",
       "      <th>insult</th>\n",
       "      <th>threat</th>\n",
       "      <th>profanity</th>\n",
       "      <th>sexually_explicit</th>\n",
       "      <th>flirtation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0776249-6b69e163629d0603a2e57c0af9b77128.txt</td>\n",
       "      <td>I’m a unionist. I believe in the Union.\\n\\nI b...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.148363</td>\n",
       "      <td>0.080491</td>\n",
       "      <td>0.402546</td>\n",
       "      <td>0.231428</td>\n",
       "      <td>0.262120</td>\n",
       "      <td>0.124949</td>\n",
       "      <td>0.146768</td>\n",
       "      <td>0.364588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0588122-39c89eab36c8e7ffbb186065859a61ea.txt</td>\n",
       "      <td>PC leadership hopefuls bring little to race\\n\\...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.148575</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0716632-aeaed959671fe280ac6d11d3b66f594e.txt</td>\n",
       "      <td>But not because of the reasons you may believe...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.143574</td>\n",
       "      <td>0.062929</td>\n",
       "      <td>0.267010</td>\n",
       "      <td>0.181736</td>\n",
       "      <td>0.328784</td>\n",
       "      <td>0.108916</td>\n",
       "      <td>0.178888</td>\n",
       "      <td>0.432094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0899501-3b54556f829125713e427e6509a713e6.txt</td>\n",
       "      <td>Foxtons has lost its second legal case in two ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.080490</td>\n",
       "      <td>0.047757</td>\n",
       "      <td>0.155986</td>\n",
       "      <td>0.109512</td>\n",
       "      <td>0.261333</td>\n",
       "      <td>0.090906</td>\n",
       "      <td>0.133572</td>\n",
       "      <td>0.373036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0000584-6470f33fa1e68138978182728c22fb4d.txt</td>\n",
       "      <td>The suspect in the apparent murder of a 22-yea...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.065222</td>\n",
       "      <td>0.069232</td>\n",
       "      <td>0.244023</td>\n",
       "      <td>0.141029</td>\n",
       "      <td>0.472590</td>\n",
       "      <td>0.142407</td>\n",
       "      <td>0.245892</td>\n",
       "      <td>0.415427</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       filename  \\\n",
       "0  0776249-6b69e163629d0603a2e57c0af9b77128.txt   \n",
       "1  0588122-39c89eab36c8e7ffbb186065859a61ea.txt   \n",
       "2  0716632-aeaed959671fe280ac6d11d3b66f594e.txt   \n",
       "3  0899501-3b54556f829125713e427e6509a713e6.txt   \n",
       "4  0000584-6470f33fa1e68138978182728c22fb4d.txt   \n",
       "\n",
       "                                                text  chunk_num  toxicity  \\\n",
       "0  I’m a unionist. I believe in the Union.\\n\\nI b...          0  0.148363   \n",
       "1  PC leadership hopefuls bring little to race\\n\\...          0  0.148575   \n",
       "2  But not because of the reasons you may believe...          0  0.143574   \n",
       "3  Foxtons has lost its second legal case in two ...          0  0.080490   \n",
       "4  The suspect in the apparent murder of a 22-yea...          0  0.065222   \n",
       "\n",
       "   severe_toxicity  identity_attack    insult    threat  profanity  \\\n",
       "0         0.080491         0.402546  0.231428  0.262120   0.124949   \n",
       "1              NaN              NaN       NaN       NaN        NaN   \n",
       "2         0.062929         0.267010  0.181736  0.328784   0.108916   \n",
       "3         0.047757         0.155986  0.109512  0.261333   0.090906   \n",
       "4         0.069232         0.244023  0.141029  0.472590   0.142407   \n",
       "\n",
       "   sexually_explicit  flirtation  \n",
       "0           0.146768    0.364588  \n",
       "1                NaN         NaN  \n",
       "2           0.178888    0.432094  \n",
       "3           0.133572    0.373036  \n",
       "4           0.245892    0.415427  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_df_raw = pd.read_pickle(TOXICITY_SCORES_PICKLE)\n",
    "true_df_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['toxicity', 'severe_toxicity', 'identity_attack', 'insult', 'threat']\n",
    "\n",
    "# Drop NA rows (old API calls) and chunk_num col (large API calls)\n",
    "true_df = true_df_raw.dropna()\n",
    "del true_df['chunk_num']\n",
    "\n",
    "# Remove all chunked data\n",
    "true_df = true_df[~true_df.duplicated(subset='filename')]\n",
    "\n",
    "# Remove columns not in pred_df\n",
    "true_df = true_df[['filename', 'text', *labels]]\n",
    "\n",
    "# Rename pred_df columns\n",
    "pred_df.rename(columns={\n",
    "    \"toxic\": \"toxicity\",\n",
    "    \"severe_toxic\": \"severe_toxicity\",\n",
    "    \"threat\": \"threat\",\n",
    "    \"insult\": \"insult\",\n",
    "    \"identity_hate\": \"identity_attack\"\n",
    "}, inplace=True)\n",
    "pred_df = pred_df[['filename', *labels]]\n",
    "\n",
    "# Inner join on filename\n",
    "merged_df = true_df.merge(pred_df, on='filename', suffixes=('_true', '_pred'))\n",
    "\n",
    "# Split and binarize labels\n",
    "y_true = merged_df[[l + '_true' for l in labels]] > 0.5\n",
    "y_pred = merged_df[[l + '_pred' for l in labels]]\n",
    "y_pred_binary = y_pred > 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_auc_score, classification_report, multilabel_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8957028472918864"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_true, y_pred_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'toxicity': 0.8640192605222321,\n",
       " 'severe_toxicity': 0.8368362267292841,\n",
       " 'identity_attack': 0.7510143725259711,\n",
       " 'insult': 0.8378905527190207,\n",
       " 'threat': 0.7519223262803563}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(zip(labels, roc_auc_score(y_true, y_pred, average=None)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "       toxicity       0.65      0.09      0.16     17750\n",
      "severe_toxicity       1.00      0.00      0.00     11527\n",
      "identity_attack       1.00      0.00      0.00     44692\n",
      "         insult       0.96      0.01      0.02     27681\n",
      "         threat       1.00      0.00      0.00     45259\n",
      "\n",
      "      micro avg       0.68      0.01      0.03    146909\n",
      "      macro avg       0.92      0.02      0.04    146909\n",
      "   weighted avg       0.95      0.01      0.02    146909\n",
      "    samples avg       0.00      0.00      0.00    146909\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true, y_pred_binary, target_names=labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[795792,    881],\n",
       "        [ 16137,   1613]],\n",
       "\n",
       "       [[802896,      0],\n",
       "        [ 11520,      7]],\n",
       "\n",
       "       [[769731,      0],\n",
       "        [ 44658,     34]],\n",
       "\n",
       "       [[786731,     11],\n",
       "        [ 27408,    273]],\n",
       "\n",
       "       [[769164,      0],\n",
       "        [ 45253,      6]]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multilabel_confusion_matrix(y_true, y_pred_binary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

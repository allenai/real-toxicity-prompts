{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/homes/gws/sgehman/language-model-toxicity\n"
     ]
    }
   ],
   "source": [
    "# HACK: use project root as the working directory \n",
    "from pathlib import Path\n",
    "\n",
    "while Path.cwd().name != 'language-model-toxicity':\n",
    "    %cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import tempfile\n",
    "\n",
    "import dask\n",
    "import dask.array as da\n",
    "from joblib import Memory, Parallel, delayed\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from utils.constants import DATA_DIR, OUTPUT_DIR\n",
    "\n",
    "# Create joblib memory\n",
    "mem = Memory(OUTPUT_DIR / 'cache' / 'webtext_overlap')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "EOS = 50256\n",
    "vocab_size = EOS + 1\n",
    "\n",
    "def bpe_files(bpe_dir: Path) -> List[Path]:\n",
    "    return [file for file in bpe_dir.iterdir() if file.suffix == '.npy']\n",
    "\n",
    "def load_meta(bpe_dir: Path):\n",
    "    files = bpe_files(bpe_dir)\n",
    "    meta = [(np.count_nonzero(array == EOS) - 1, array.dtype)\n",
    "            for array \n",
    "            in tqdm(map(np.load, files), total=len(files), desc='Loading meta')]\n",
    "    shapes, dtypes = zip(*meta)\n",
    "    return files, shapes, dtypes[0]\n",
    "\n",
    "# Cache calls to load_meta\n",
    "load_meta = mem.cache(load_meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "wt_dir = DATA_DIR / 'webtext'\n",
    "wt_meta = load_meta(wt_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "owtc_dir = DATA_DIR / 'openwebtext_bpe'\n",
    "owtc_meta = load_meta(owtc_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "owtc_files = owtc_meta[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_docs(tokens: np.array) -> np.array:\n",
    "    idx = np.nonzero(tokens == EOS)[0]\n",
    "    docs = np.split(tokens, idx)\n",
    "    docs = [doc[1:] for doc in docs if len(doc) > 1]\n",
    "    return np.array(docs)\n",
    "\n",
    "def load_corpus_into_memory(files: List[Path]):\n",
    "    corpus = []\n",
    "    for shard in tqdm(map(np.load, files), total=len(files)):\n",
    "        corpus.extend(split_docs(shard))\n",
    "    return corpus\n",
    "\n",
    "delayed_load = dask.delayed(lambda f: split_docs(np.load(f)))\n",
    "\n",
    "def load_corpus(meta):\n",
    "    files, shapes, dtype = meta\n",
    "    \n",
    "    # Create delayed arrays\n",
    "    delayed_arrays = list(map(delayed_load, files))\n",
    "        \n",
    "    # Concatenate arrays\n",
    "    corpus = da.concatenate([da.from_delayed(array, shape=(shape,), dtype=dtype) \n",
    "                             for array, shape in zip(delayed_arrays, shapes)])\n",
    "\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "from sklearn.feature_extraction.text import HashingVectorizer, CountVectorizer\n",
    "\n",
    "def _load_shard(file: Path):\n",
    "    print(\"Loading shard:\", file.stem)\n",
    "    shard = np.load(file)\n",
    "    print(\"Splitting documents:\", file.stem)\n",
    "    docs = split_docs(shard)\n",
    "    return docs\n",
    "\n",
    "def load_corpus_vectors(files: List[Path], n_jobs: int):\n",
    "    with Parallel(n_jobs=n_jobs) as parallel:\n",
    "        print(\"Loading shards...\")\n",
    "        shards = parallel(\n",
    "            delayed(_load_shard)(file) for file in files\n",
    "        )\n",
    "\n",
    "        print(\"CountVectorizing...\")\n",
    "        identity = lambda x: x\n",
    "        vectorizer = CountVectorizer(vocabulary=range(EOS + 1), analyzer=identity)\n",
    "\n",
    "        vectorized_docs = parallel(\n",
    "            delayed(vectorizer.transform)(shard) for shard in shards\n",
    "        )\n",
    "    \n",
    "    return vectorized_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import vstack, save_npz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading shards...\n",
      "CountVectorizing...\n"
     ]
    }
   ],
   "source": [
    "vectorized_docs = load_corpus_vectors(owtc_files, n_jobs=20)\n",
    "vectorized_docs = vstack(vectorized_docs)\n",
    "save_npz(OUTPUT_DIR / 'vecs', vectorized_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HACK: use project root as the working directory \n",
    "from pathlib import Path\n",
    "\n",
    "while Path.cwd().name != 'language-model-toxicity':\n",
    "    %cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import multiprocessing as mp\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "from datasketch import MinHash, LeanMinHash, MinHashLSH\n",
    "from joblib import load\n",
    "from nltk import ngrams\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from utils.constants import DATA_DIR, OUTPUT_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MinHash()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_corpus_iter(corpus_dir: Path):\n",
    "    files = sorted([file for file in corpus_dir.iterdir() if file.suffix == '.joblib'])\n",
    "\n",
    "    i = 0\n",
    "    for file in files:\n",
    "        docs = load(file)\n",
    "\n",
    "        # Load filenames or ids\n",
    "        filenames_file = file.with_name(f'{file.stem}_filenames.txt')\n",
    "        doc_ids = (\n",
    "            filenames_file.read_text().split()\n",
    "            if filenames_file.exists()\n",
    "            else map(lambda idx: f'{file.stem}-{idx}', range(len(docs)))\n",
    "        )\n",
    "\n",
    "        print(\"Loading file:\", file)\n",
    "        for doc_id, doc in zip(doc_ids, docs):\n",
    "            # Yield name and doc\n",
    "            yield doc_id, doc\n",
    "            i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_minhash_mapping(item, shingles: int, num_perm: int):\n",
    "    doc_id, doc = item\n",
    "    \n",
    "    # Create MinHash\n",
    "    shingles_set = set(ngrams(doc, 5))\n",
    "    m = MinHash(num_perm=num_perm)\n",
    "    for s in shingles_set:\n",
    "        s = ''.join(s).encode('utf8')\n",
    "        m.update(s)\n",
    "     \n",
    "    # Convert to LeanMinHash\n",
    "    m = LeanMinHash(m)\n",
    "\n",
    "    return doc_id, m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallel_create_minhashes(corpus_iter, shingles: int, num_perm: int, n_jobs: int, chunksize = 1000):\n",
    "    make_minhash_mapping_ = partial(make_minhash_mapping, shingles=shingles, num_perm=num_perm)\n",
    "    \n",
    "    with mp.Pool(n_jobs) as pool:\n",
    "        yield from pool.imap(make_minhash_mapping_, corpus_iter, chunksize=chunksize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create MinHashLSH for WebText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_PERM = 128\n",
    "SHINGLES = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JACCARD = 0.9\n",
    "lsh = MinHashLSH(threshold=JACCARD, num_perm=NUM_PERM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wt_len = 8_282_020\n",
    "wt_iter = make_corpus_iter(DATA_DIR / 'detokenized_webtext')\n",
    "\n",
    "mh_iter = parallel_create_minhashes(wt_iter, total=wt_len, shingles=SHINGLES, num_perm=NUM_PERM, n_jobs=96)\n",
    "wt_minhashes = {}\n",
    "\n",
    "with lsh.insertion_session() as session:\n",
    "    for key, minhash in tqdm(mh_iter, total=wt_len):\n",
    "        wt_minhashes[key] = minhash\n",
    "        session.insert(key, minhash, check_duplication=False)  # All keys are unique doc ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create MinHashes for OpenWebText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create MinHash LSH with WebText\n",
    "with mp.Pool(96) as pool:\n",
    "    with lsh.insertion_session() as session:\n",
    "        for key, minhash in tqdm(mh_iter, total=wt_len):\n",
    "            session.insert(key, minhash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(OUTPUT_DIR / 'datasketch_v1.pkl', 'wb') as f:\n",
    "    pickle.dump(lsh, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "owtc_len = 8_013_769\n",
    "owtc_dir = DATA_DIR / 'openwebtext_shards'\n",
    "owtc_iter = make_corpus_iter(owtc_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "owtc_minhashes = {}\n",
    "\n",
    "# Create MinHashes for OWTC\n",
    "with mp.Pool(96) as pool:\n",
    "    mh_iter = pool.imap(make_minhash_, owtc_iter, chunksize=1000)\n",
    "    for key, minhash in tqdm(mh_iter, total=owtc_len):\n",
    "        owtc_minhashes[key] = minhash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST\n",
    "test_id, test_mh = make_minhash_(next(owtc_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_id, test_mh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_matches = lsh.query(test_mh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
